# Stock-Price-Monitoring

The project's aim is to automatically fetch stock data from the NSE. Currently, it can fetch any stock's dividends, events, 3 months' data, and yesterday's/today's data provided that the current trading session has ended and the data has been uploaded on the website.
The project is build **LangChain** framework which is used to develop **RAG(Retrival Augmented Generation)** based systems.
If you are unaware about any of the following concepts do check these short video by IBM.
langChain - https://youtu.be/1bUy-1hGZpI?si=qaA2Qabx6nk4FqCT
RAG - https://youtu.be/T-D1OfcDW1M?si=6DfIQx-IQ0tUD6V0

<br />

# LLM Used in the Project

1) **Mixtral-8x7B** – Used as the main LLM to answer user queries regarding stocks, such as opening price, closing price, previous closing, volume traded, value of the traded volume, etc., and for comparing values of two different days.  
2) **nomic-embed-text-v2** – Used to create word embeddings of the data collected from the NSE and user input to enable efficient searching in the database.

# Database

The project uses **ChromaDB** to store all the vector embeddings.

## The Project is Divided into 3 Segments -

### 1) Scraping the Data

- The project uses **Selenium** to scrape data from the NSE site. Selenium is an open-source framework primarily used for testing web applications, but it can also perform web scraping. It requires a ChromeDriver matching your current Chrome version installed on your computer.  

- This repo contains two ChromeDriver versions:  
  - `chromedriver.exe` → Version 134  
  - `chromedriver2.exe` → Version 137  

If you have a different Chrome version, download the matching ChromeDriver and place it in your `Scrapping` folder. To check your Chrome version, type the following in your Chrome address bar:  

```
chrome://settings/help

```

- **Scrape.py**  
  This script extracts information from the NSE website. The class `StockInformation` is called inside `WebScrapper.py`. It also formats the scraped information into sentence form to save in a text file, which can later be used in a vector database after performing word embedding.  

  The sentence format will look something like this:  

```
On 27-May-2025, NESTLEIND's stock opened at 2,450.10, reached a high of 2,482.00 and a low of 2,432.20. The previous day's closing price was 2,454.00, and on 27-May-2025 it closed at 2,460.90. The traded volume was 7,92,049, with 86,090 trades and total value traded was 1,94,38,48,033.60..

```

- **WebScrapper.py**  
Contains a list of companies whose data you want to scrape. It also disables import messages to prevent the NSE servers from detecting that the traffic is generated by a web bot, as NSE restricts access by automated bots.

- **add_to_mongo.py**  
Helps in adding the scraped data to MongoDB, which is used later to display charts about stock prices on the website.

### 2) ChatBot

This folder serves two primary purposes:  
1. Performing word embeddings and storing them in the vector database.  
2. Running a chatbot that takes user queries, embeds them using the same embedding model, and performs a similarity search in the vector store to retrieve relevant information.

---

####  Word Embedding

The project uses the **`RecursiveCharacterTextSplitter`** from **LangChain** to split raw text into multiple manageable chunks before embedding. This process is essential and offers several benefits:

1. **Handles non-uniform document lengths** — Ensures consistent chunk sizes for better downstream processing.  
2. **Overcomes model limitations** — Many embedding and language models have maximum token limits; chunking allows even long documents to be processed effectively.  
3. **Improves embedding quality** — Shorter, focused chunks lead to better semantic representations.  
4. **Enhances retrieval precision** — Enables more accurate similarity matching during searches.  
5. **Optimizes resource usage** — Reduces memory and compute load by avoiding massive text blobs.

The `RecursiveCharacterTextSplitter` also uses a parameter called `chunk_overlap`, which overlaps a portion of the previous chunk with the next. This overlap helps preserve context across chunk boundaries, ensuring coherent semantic understanding.


---

